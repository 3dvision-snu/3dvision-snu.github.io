<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>3D Vision Lab - Research</title>
  <meta name="description" content="3D Vision Lab -- Research">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/research/">
<link rel="shortcut icon" type ="image/x-icon" href="/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
    <a class="navbar-brand" href="/">3D Vision Lab</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="/">Home</a></li>
		<li><a href="/members">Members</a></li>
		<li><a href="/publications">Publications</a></li>
		<li><a href="/class">Class</a></li>
		<li><a href="/research">Research</a></li>
		<li><a href="/openings">Openings</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid content">
      <div class="row">
        <div id="textid" class="col-sm-12">
  <h2 id="research">Research</h2>
<p><br />
We strive to the emerging field of 3D vision where we understand the 3D world around us. We not only sense, acquire and perceive the 3D models, but also visualize and extract semantic information to develop various applications, namely VR/AR, robotics, human augmentation, and ambient intelligence to name a few. The ultimate goal is the bridge between human and intelligent agent to benefit human.</p>

<h3 id="research-projects-include-">Research projects include: <br /></h3>
<p><strong style="font-size: 110%">1) 3D acquisition and modeling:</strong> With the help of various acquisition devices, we can capture and build a 3D model of objects, scenes, and human bodies. The research assist robots or human users to acquire the 3D models of interest. We also ease the everyday use of acquired models in everyday life by updating the 3D models with light-weight acquisition devices, or send the stream of 3D video over mobile network.</p>

<p><img src="/images/respic/image001.png" alt="" style="width: 300px; border: 10px" /><br />
<img src="/images/respic/image002.png" alt="" style="width: 300px; border: 10px" /><br /></p>

<p><strong>Light-Weight Update of Large- Scale 3D Scenes</strong><br />
<u>Research objective</u>: Given off-line 3D scan and online 360 photo, quickly update the 3D model of the scene.<br />
<u>Approach</u>: Hybrid feature matching of different data modality for localization, the change detection of 360 images with respect to the large- scale 3D model.<br /></p>

<p><img src="/images/respic/image003.png" alt="" style="width: 300px; border: 10px" /><br /></p>

<p><strong>Real-Time 3D Video Streaming System</strong><br />
<u>Research objective</u>: Streaming volumetric video at video frame rate via mobile devices<br />
<u>Research scope</u>: Efficient storing and processing with dynamic octree structure, adaptive rendering resolution and view-point prediction<br /><br /></p>

<p><strong style="font-size: 110%">2) Using 3D models for visualization:</strong> 3D models are crucial for seamless AR/VR applications and realistic rendering. The key technical components include localization, pose estimation, texture acquisition, and lighting estimation.<br />
<img src="/images/respic/image004.jpg" alt="" style="width: 300px; border: 10px" /><br />
<img src="/images/respic/image005.png" alt="" style="width: 300px; border: 10px" /><br /></p>

<p><strong>Novel View Synthesis</strong><br />
<u>Research objective</u>: Synthesize a target view from a given source view and its camera pose without a 3D model, Extend the latent space representation into multi-object 3D scenes<br />
<u>Approach</u>: Neural rending with end-to-end trainable framework<br /><br /></p>

<p><strong style="font-size: 110%">3) Using 3D data for perception:</strong> 3D information is widely used for perception, and recent trends on 3D perception utilize the state-of-the-art techniques from computer vision and machine learning. We utilize neural networks of generative models, metric learning, and/or reinforcement learning to boost the performance.<br /></p>

<p><img src="/images/respic/image006.jpg" alt="" style="width: 300px; border: 10px" /><br />
<img src="/images/respic/image007.png" alt="" style="width: 300px; border: 10px" /><br /></p>

<p><strong>Shape Completion Preserving Details</strong><br />
<u>Motivation</u>: Existing 3D data suffer from occlusion and noise of sensors. Current shape completion pipeline acts as object recognition<br />
<u>Approach</u>: Train GAN in local and global shape and use graph-based convolutional network to preserve details<br /></p>

<p><img src="/images/respic/image008.png" alt="" style="width: 300px; border: 10px" /><br /></p>

<p><strong>Point Cloud Instance Segmentation</strong><br />
<u>Research objective</u>: Direct segmentation of individual objects on the raw 3D measurements<br />
<u>Approach</u>: Deep metric learning on point cloud feature extracted via sparse convolutional neural network<br /><br /></p>

<p><strong>4) Using 3D information for interaction</strong>:  The real-world 3D models are utilized for manipulation, navigation, or other robotic applications. Our focus is to build algorithms to control robot in un-constrained set-up and robust to small changes.</p>

<p><img src="/images/respic/image009.png" alt="" style="width: 300px; border: 10px" /><br /></p>

<p><strong>Visuomotor Policy via Scene Understanding</strong><br />
<u>Research objective</u>: Given complex visual scene, make robot disentangle raw observation into latent full state, and learn generalized policy.<br />
<u>Research scope</u>: Robot manipulation in non-stationary environment based on 2D/3D raw observation<br /></p>

<p><img src="/images/respic/image010.png" alt="" style="width: 300px; border: 10px" /><br /></p>

<p><strong>Occlusion-Aware Navigation</strong><br />
<u>Research objective</u>: Given an approximate map of changing environment, infer the navigation path allowing online modification<br />
<u>Research scope</u>: Scene understanding from partial observation, joint inference of human-object interaction and scene reconstruction<br /><br /></p>

</div>

      </div>
    </div>

    <div class="panel-info" style = "background-color: black">
  <div class="container-fluid">
      <div class="row">
        <div class="col-md-8" style = " font-size:0.8em;  color: darkgrey;">  Copyright 2020, 3D Vision Laboratory, Dept. of Electrical and Computer Engineering, Seoul National University. </div>
        <div class="col-md-8" style = " font-size:0.8em;  color: darkgrey;">
          Contact: Room 916, Building 301, 1 Gwanak-ro, Gwanak-gu, Seoul, Republic of Korea
        </div>
      </div>
  </div>
</div>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/custom_style.js"></script>

  </body>

</html>
